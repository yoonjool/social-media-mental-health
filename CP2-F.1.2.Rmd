---
title: "CDS 101 Final Project"
author: "Group 3"
date: "`r Sys.Date()`"
documentclass: article
geometry: margin=1in
fontsize: 11pt
output:
  pdf_document:
    highlight: tango
    toc: false
    df_print: kable
    fig_caption: no
    number_sections: no
    dev: pdf
    latex_engine: xelatex
# mainfont: "Malgun Gothic"
---

```{r setup, include = FALSE}
# Set knitr options
knitr::opts_chunk$set(
  echo = TRUE, eval = TRUE, fig.width = 6, warning = FALSE,
  message = FALSE, fig.asp = 0.618, out.width = "80%", dpi = 120,
  fig.align = "center", cache = FALSE
)

# Load required packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(broom))
suppressPackageStartupMessages(library(modelr))
suppressPackageStartupMessages(library(plotly))
suppressPackageStartupMessages(library(infer))

# Load Cohen's d bootstrap helper functions
load("bootstrap_cohens_d.RData")

# Load dataset
data = readr::read_csv("smmh.csv")
colnames(data)

# Set seed
set.seed(203904)
```

## Yoonjoo Lee

# Remove unwanted columns

* Unwanted columns
  * Timestamp
  * Relationship Status
  * Occupation Status
  * What type of organizations are you affiliated with?
  * Do you use social media?
  * What social media platforms do you commonly use?
  * Following the previous question, how do you feel about these comparisons, 
  generally speaking?
 
* Generate a new dataset called "data_r" excluding Unwanted columns 
in the original dataset called 'data".

```{r}
data_r <- data %>%
  select(-1, -4, -5, -6, -7, -8, -17)
```


# Calculate the average dependence on social media

* Selected columns
  * How often do you find yourself using Social media without a specific purpose?
  * How often do you get distracted by Social media when you are busy doing 
  something?
  * Do you feel restless if you haven't used Social media in a while?
  * How often do you look to seek validation from features of social media?
  
* Create a new column called "dependence" that is the average of the selected columns.

```{r}
data_r <- data_r %>%
  mutate(dependence = rowMeans(select(., c(4, 5, 6, 11)), na.rm = TRUE))
```

# Calculate the average social media distraction

* Selected columns
  * On a scale of 1 to 5, how easily distracted are you?
  * Do you find it difficult to concentrate on things?
* Create a new column called "distraction" that is the average of the selected columns.


```{r}
data_r <- data_r %>%
  mutate(distraction = rowMeans(select(., c(4,6)), na.rm = TRUE))
```


# Calculate the average mental health

* Selected columns
  * On a scale of 1 to 5, how much are you bothered by worries?
  * How often do you feel depressed or down?
  * How frequently does your interest in daily activities fluctuate?
  * On a scale of 1 to 5, how often do you face issues regarding sleep?

* Create a new column called "mental_health" that is the average of the selected columns.

* The higher the "mental_health" value, the more negative mental health is.

```{r}
data_r <- data_r %>%
  mutate(mental_health = rowMeans(select(., c(4,6,7,8)), na.rm = TRUE))
```


# Rename function to change the name of columns

* This code aims to simplify and make the column names more straightforward and easily understandable.

```{r}
data_r <- data_r %>%
  rename(age = `1. What is your age?`,
         gender = `2. Gender`,
         avgtime = `8. What is the average time you spend on social media every day?`,
         comparison =
'15. On a scale of 1-5, how often do you compare yourself to other successful people through the use of social media?'
         )
```

# Convert numerical data to continuous data

* This codes aims to change categorical data to numerical data for some plots that require numerical data. Plus, the new column "avgtime_numeric" can simplify the representation of the average time spent on social media, making it easier to work with in data analysis.

```{r}
data_r <- data_r %>%
  mutate(avgtime_numeric = case_when(
    avgtime == "Less than an Hour" ~ 0.5,
    avgtime == "Between 1 and 2 hours" ~ 1.5,
    avgtime == "Between 2 and 3 hours" ~ 2.5,
    avgtime == "Between 3 and 4 hours" ~ 3.5,
    avgtime == "Between 4 and 5 hours" ~ 4.5,
    avgtime == "More than 5 hours" ~ 5.5
  ))
```

# Change the name of rows

* There are other genders in the gender column, excluding women and men. Change all their row names to others.

```{r}
data_r <- data_r %>%
  mutate(gender = ifelse(gender %in% c("NB","Non-binary","Non binary","Nonbinary","There are others???","Trans","unsure"), "others", gender))
```


## Hazel Park

# Visualize the age distribution

```{r}
data_r %>%
  ggplot() +
  geom_histogram(mapping = aes(x=age), binwidth=3, color="darkgreen"
              ,fill="lightgreen")+
 
  labs(x="age", y ="count", title="Histogram for age") +
   xlim(15, 60)
```

* The histogram has a single prominent peak, which is unimodal modality.
* The histogram displays a "right-skewed" skewness.
* The highest peak is located in the "early 20s."

**Therefore, this dataset has a high frequency mainly in the early 20s age group.**


# Visualize the gender distribution

```{r}
data_r %>%
  ggplot() +
  geom_bar(mapping = aes(x=gender), fill='yellow', color='black') +
  labs(x='gender', y='count', title='Bar graph for gender')
```
**We can see from this bar graph that the people we explore have more female genders.**


# Visualize the average time distribution

```{r}
data_r %>%
  ggplot() +
  geom_histogram(mapping = aes(x=avgtime_numeric), binwidth=1.3, color="darkblue"
                 ,fill="lightblue")+
  labs(x="the average time", y ="count", title="Histogram for average time spent 
       on social media")
```
* The histogram has a single prominent peak, which is unimodal modality.
* The histogram displays a "left-skewed" skewness.
* The highest peak is almost at almost 4 hours.


**This histogram indicates that the average time spent most on social media is approximately 4 hours.**


# Visualize the relationship between distraction and average time


```{r, echo=FALSE, fig.height=7, fig.width=8}
data_r %>%
  ggplot() +
  geom_boxplot(mapping = aes(x=distraction, y=avgtime))+
  labs(x="social media distraction", y ="average time spent on social media"
       , title="Boxplot social media usage time according to social media distraction level")
```


**Visually, as the average time increases, we can observe a corresponding rise in the level of social media distraction.**


# Visualize the relationship between dependence and average time

```{r, echo=FALSE, fig.height=7, fig.width=8}
data_r %>%
  ggplot() +
  geom_violin(mapping = aes(x=dependence, y=avgtime))+
  labs(x="dependence on social media", y ="the average time spent on social media"
       , title="Violin plot of social media usage time according to dependence 
       on social media")
```

* The groups that spend more time on social media have longer tails.
* This violin has a long right tail.
* A stark difference between 'More than 5 hours' and  'Less than an hour'


# Relationship between time spent on social media and comparison with others

```{r}
data_r %>%
  ggplot() +
  geom_smooth(mapping = aes(x = avgtime_numeric, y = comparison)) +
  labs(x = "average time spent on social media", y = "comparison with others"
       , title = "Trend line of social media usage time and comparison with others")
```

* A positive correlation (the trend line slopes upward)
* The line is on an increasing trend.

**As this line is on an increasing trend, we can see that as the average time spent on social media increases, the index of comparison to others through social media increases.**


# Relationship between time spent on social media and mental health

```{r}
data_r %>%
  ggplot() +
  geom_histogram(mapping = aes(x = mental_health, fill = avgtime), bins = 5, alpha = 0.9, position = "identity") +
  labs(x = "mental health", y = "count", fill = "time spent on social media"
       , title= "Histogram of mental health by social media usage time")
```

* To see the frequency or count of observations within different ranges of mental health.
* Pink (More than 5 hours is skewed to negative mental levels of 3, 4, and 5
* The highest peak is at 3 hours.


# Relationship between time spent on social media and mental health

```{r}
data_r %>%
  ggplot() +
  geom_smooth(mapping = aes(x = avgtime_numeric, y = mental_health)) +
  labs(x="average time spent on social media", y ="mental health"
       , title="Scatter Plot of time spent on social media vs mental health")
```
* The trend line slopes gradually upward.

**we can know as the average time spent on social media increases, mental health tends to deteriorate.**


###   Sangwon Yum (Module 5 & 8)

# Answers about CheckPoint 1's Question
- Q) What functions do you use to import and transform data? 
What conversions will you be doing?

- A) For data reading, use the read_csv function in the readr package. 
When converting data, use mutate (add column) and rename (change column name) 
in the dplyr package, and especially convert avgtime from a simple string to 
numeric value through case_when (avgtime string -> avgtime_numeric numeric value).

- Q) What are you modeling in linear regression?

- A) To predict the dependent variable (avgtime_numeric), select independent 
variables (age, comparison, dependence, distraction, mental_health) and perform a linear regression model.

- Q) Facet wrapping doesn't tell you whether one variable is affected by another 
variable.

- A) To understand the relationship between independent and dependent variables, 
visualize the scatter plot matrix using the ggpairs function in the GGally package. 
In particular, the bottom left shows a scatter plot between each variable, 
the diagonal shows the frequency distribution, and the top right shows correlation 
coefficients and significance test results.

- Q) Linear regression doesn't tell you about statistical significance.

- A) The modified coefficient of determination in the optimal model is 0.33 
(33% explanatory power compared to the total variance), showing statistically 
significant results at the significance level of 0.05 or less. In addition, 
for each regression coefficient, the negative relationship (age) in the 
correlation analysis showed -0.04, while dependence and mental_health are 
positive relationships and are statistically significant. Previously, 
cross-validation was performed on the optimal significant model, and as a result,
the root mean square error (RMSE) was 1.27, showing a slight error.

- Q) Hypothesis testing does not prove whether the research question is true or 
not (there is no such thing as a hypothesis graph).

- A) Linearity/homoscedasticity/normality hypothesis testing and visualization 
are performed from prediction and residual results using test data. As a result 
of the linearity test, the P value is 0.5604, which is higher than the significance 
level of 0.05, so the null hypothesis cannot be rejected, so the correlation 
coefficient is 0 (no linearity). As a result of the homoscedasticity test, 
the P value is 0.5796, which is higher than the significance level of 0.05 or 
less, so the null hypothesis cannot be rejected, resulting in homoscedasticity O.
As a result of the normality test, the P value is 0.0005575, which is lower 
than the significance level of 0.05 or less, so the null hypothesis is rejected 
and normal distribution

# Correlation coefficient/significance test
- To predict avgtime, correlation coefficient and significance test between 
independent and dependent variables are performed.
- As a result, there was a high positive relationship (0.20 ~ 0.44) in the order 
of comparison, distraction, mental_health, and dependence, while age showed a 
negative relationship (-0.38).
- To test the significance of these correlation coefficients, all variables are 
statistically significant at a significance level of 0.05 or less through 
the rstatix::cor_pmat function, similar to cor.test.
- Based on the previous numerical analysis results, it is visualized and 
confirmed through the correlation coefficient matrix and dispersion matrix.
```{r, warning=FALSE}
modelData = data_r %>% 
  dplyr::select(age, comparison, dependence, distraction, mental_health, avgtime_numeric) %>% 
  as.tibble()

str(modelData)

corMat = rstatix::cor_mat(modelData)
corPmat = rstatix::cor_pmat(modelData)

# Correlation coefficient
corMat %>% 
  dplyr::select(rowname, avgtime_numeric) %>% 
  dplyr::arrange(avgtime_numeric)

# Correlation coefficient significance test
corPmat %>% 
  dplyr::select(rowname, avgtime_numeric) %>% 
  dplyr::arrange(avgtime_numeric)

# Correlation coefficient matrix
ggcorrplot::ggcorrplot(corMat, hc.order = TRUE, type = "lower", lab_col = "black"
                       , outline.color = "white", lab = TRUE, p.mat = corPmat) + 
  labs(title = 'Correlation Matrix')

# Scatter plot matrix
GGally::ggpairs(modelData) +
  labs(title = 'Pairwise Relationships')
```

# Training and data splitting
```{r, warning=FALSE}
set.seed(1)

# Set index to divide training/test data 70:30
idx = sample(1:nrow(modelData), nrow(modelData) * 0.7)

# Split data according to the index
trainData = modelData[idx, ]
testData = modelData[-idx, ]
```

# Select variables in linear regression model
- Perform a linear regression model by selecting independent variables 
(age, comparison, dependence, distraction, mental_health) and dependent variables 
(avgtime) from the training data.
- Select the statAIC variable to derive a meaningful regression model and select 
the model with the minimum validation index (AIC) (optimal meaningful model: 
avgtime_numeric ~ age + dependence + mental_health)
```{r, warning=FALSE}
# Perform linear regression model for all variables
# Independent variables: All variables except avgtime_numeric
# Dependent variable: avgtime_numeric
lmFitVarAll = lm(avgtime_numeric ~ ., data = trainData, x = TRUE, y = TRUE)

# Select variables based on AIC
rsStepAic = MASS::stepAIC(lmFitVarAll, direction = "both")

# Summary of results
summary(rsStepAic)

# Check analysis results at a glance
rsStepAic$anova
```

# Selection of optimal model and cross-validation in linear regression model
- The modified coefficient of determination in the optimal model is 0.28 
(28% explanatory power compared to the total variance), showing statistically 
significant results at the significance level of 0.05 or less.
- Also, in the case of each regression coefficient, the negative relationship 
(age) in the correlation analysis indicated -0.05, while dependence and 
mental_health are positive relationships and are statistically significant.
- Previously, cross-validation was performed on the significant optimal model, 
and as a result, the root mean square error (RMSE) was 1.35, showing a slight 
error.
```{r, warning=FALSE}
# Select the optimal model
lmBestModel = rsStepAic
summary(lmBestModel)

# Cross-validation
lmBestModelCv = lmvar::cv.lm(lmBestModel, k = 10)
rmseVal = lmBestModelCv$MSE_sqrt$mean %>% round(2)
print(rmseVal)
```

###   Jeongwoon Ka (Module 5 & 8)

# Prediction using test data
- Make predictions using test data and visualize predictions and actual 
measurements through scatter plots
```{r, warning=FALSE}
# Best model and cross-validation using test data
prdData = testData %>% 
  add_predictions(rsStepAic, var = "pred") %>%
  add_residuals(rsStepAic, var = "resid")
head(prdData)
```

# Linearity/homoscedasticity/normality hypothesis testing and visualization
- As a result of the linearity test, the P value is 0.7093, which is higher 
than the significance level of 0.05 or less, so the null hypothesis cannot be 
rejected, so the correlation coefficient is 0 (no linearity)
- As a result of the homoscedasticity test, the P value is 0.02222, which is 
lower than the significance level of 0.05 or less, so the null hypothesis is 
rejected and there is no homoscedasticity.
- As a result of the normality test, the P value is 0.02955, which is less than 
the significance level of 0.05, so the null hypothesis is rejected and there is 
no normal distribution.
```{r, warning=FALSE}
# Linearity test
prdData %>%
  ggplot(aes(x = pred, y = avgtime_numeric)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(
    title = "Observed vs Predicted Scatterplot",
    x = "Predicted Values",
    y = "Observed Values"
  )
cor.test(prdData$pred, prdData$resid)
```
-   As a result of the linearity test, the P value is 0.7093, 
which is higher than the significance level of 0.05 or less, 
so the null hypothesis cannot be rejected, so the correlation coefficient is 0 
(no linearity)

```{r, warning=FALSE}
# Test for homoscedasticity
prdData %>% 
  ggplot(aes(x = pred, y = resid)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
    labs(
      title = "Residual vs. Predicted Values Scatterplot",
      x = "Predicted Values",
      y = "Residuals"
    )
lmtest::bptest(lmBestModel)
```

-   As a result of the homoscedasticity test, the P value is 0.02222, 
which is lower than the significance level of 0.05 or less, 
so the null hypothesis is rejected and there is no homoscedasticity.
```{r, warning=FALSE}
# Normality test
prdData %>%
  ggplot(aes(sample = resid)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Q-Q Plot of resid",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")
shapiro.test(prdData$resid)
```

-   As a result of the normality test, the P value is 0.02955, 
which is less than the significance level of 0.05, 
so the null hypothesis is rejected and there is no normal distribution.

# visualization using test data
- As a result, the correlation coefficient between the two data is 0.43, 
which is statistically significant at the significance level of 0.05 or less.
- Also, the root mean square error (RMSE) verification result between predictions 
and actual measurements is 1.27, and some errors occur.
- This is judged to be the absence of learning materials for various conditions 
because the number of training data is small at 331.
# Therefore, it is believed that avgtime's prediction performance will improve if it not only collects a variety of learning data but also includes analysis variables.
```{r, warning=FALSE}
# Visualization
ggpubr::ggscatter(prdData, x = "pred", y = "avgtime_numeric") +
  ggpubr::stat_regline_equation(label.x.npc = 0.0, label.y.npc = 0.95, 
                                color = "blue", size = 4.5) +
  ggpubr::stat_cor(label.x.npc = 0.0, label.y.npc = 0.85, color = "blue", 
                   size = 4.5) +
  annotate("text", x = 0, y = 4.2, size = 4.5, 
           label = sprintf("RMSE = %s", rmseVal), color = "blue", hjust = 0, 
           fontface = "italic") +
  geom_abline(intercept = 0, slope = 1, linetype = 1, color = "red", size = 1.0) +
  theme_bw() +
  labs(title = "Prediction Results", x = "Predicted", y = "Observed") +
  theme(text = element_text(size = 16))
```

## Jaewon Han and Jiwon Park (Module 6,7)
## Data Transformation
```{r}
data_r <- data_r %>%
  mutate(Average_Usage = case_when(avgtime_numeric < 3.5 ~ 'LOW', 
                                   avgtime_numeric >= 3.5 ~ 'HIGH'))
```

## Data Filtering & Summary Statistics
```{r}
data_clean <- data_r %>%
  filter(Average_Usage == "HIGH" | Average_Usage == "LOW")
data_clean %>%
  group_by(Average_Usage) %>%
  summarise(mean = mean(mental_health),
            median = median(mental_health),
            sd = sd(mental_health),
            iqr = IQR(mental_health),
            min = min(mental_health),
            max = max(mental_health))
```

## Install infer package
```{r}
library(infer)
```

## Permutaion Test for independence
```{r}
data_null <- data_clean %>%
  specify(mental_health ~ Average_Usage) %>%
  hypothesise(null = "independence") %>%
  generate(reps = 10000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("HIGH", "LOW"))
```

## Calculate observed Statistic
```{r}
data_obs <- data_clean %>%
  specify(formula = mental_health ~ Average_Usage) %>%
  calculate (stat = "diff in means", order = c("HIGH", "LOW"))
```

## Calculate and Visualize P-value
```{r}
data_clean %>%
  get_p_value(obs_stat = data_obs, direction = "less")
```

## Visualize the results of hypothesis test
```{r}
data_null %>%
  visualize() +
  shade_p_value(obs_stat = data_obs, direction = "less") +
  labs(
    title = "Hypothesis test with Average Usage and Mental Health",
    x= "Mean gap",
    y= "Count")
```
i. As the P-value is smaller than Alpha, we reject the null hypothesis in favor 
of the alternative hypothesis. There is a relationship between time spent 
on social media and mental health.

## Visualize the results of hypothesis test
```{r}
data_bootstraps <- data_clean %>%
  specify(mental_health ~ Average_Usage) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "diff in means", order = c("HIGH", "LOW"))
```

```{r}
bootstrap_ci <- data_bootstraps %>%
  get_confidence_interval()
```

```{r}
bootstrap_ci
```

## Visualize Bootstrap Results
```{r}
data_bootstraps %>%
  visualize() + shade_confidence_interval(bootstrap_ci) + labs(
    title = "Bootstrap with Confidence Interval",
    x = "Mean Gap",
    y = "Count")
```

## Cohenâ€™s d Bootstrap
```{r}
bootstrap_results <- cohens_d_bootstrap(
  data = data_r,
  model = mental_health ~ Average_Usage)
```

## Bootstrap Report
```{r}
bootstrap_report(bootstrap_results)
```

## Bootstrap Visualization
```{r}
plot_ci(bootstrap_results)
```
